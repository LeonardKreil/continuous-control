@article{
doi:10.1126/science.aar6404,
author = {David Silver  and Thomas Hubert  and Julian Schrittwieser  and Ioannis Antonoglou  and Matthew Lai  and Arthur Guez  and Marc Lanctot  and Laurent Sifre  and Dharshan Kumaran  and Thore Graepel  and Timothy Lillicrap  and Karen Simonyan  and Demis Hassabis },
title = {A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
journal = {Science},
volume = {362},
number = {6419},
pages = {1140-1144},
year = {2018},
doi = {10.1126/science.aar6404},
URL = {https://www.science.org/doi/abs/10.1126/science.aar6404},
eprint = {https://www.science.org/doi/pdf/10.1126/science.aar6404},
abstract = {Computers can beat humans at increasingly complex games, including chess and Go. However, these programs are typically constructed for a particular game, exploiting its properties, such as the symmetries of the board on which it is played. Silver et al. developed a program called AlphaZero, which taught itself to play Go, chess, and shogi (a Japanese version of chess) (see the Editorial, and the Perspective by Campbell). AlphaZero managed to beat state-of-the-art programs specializing in these three games. The ability of AlphaZero to adapt to various game rules is a notable step toward achieving a general game-playing system. Science, this issue p. 1140; see also pp. 1087 and 1118 AlphaZero teaches itself to play three different board games and beats state-of-the-art programs in each. The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.}}

@inproceedings{NEURIPS2018_059fdcd9,
 author = {Lazic, Nevena and Boutilier, Craig and Lu, Tyler and Wong, Eehern and Roy, Binz and Ryu, MK and Imwalle, Greg},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Data center cooling using model-predictive control},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/059fdcd96baeb75112f09fa1dcc740cc-Paper.pdf},
 volume = {31},
 year = {2018}
}

@misc{lillicrap2019continuouscontroldeepreinforcement,
      title={Continuous control with deep reinforcement learning}, 
      author={Timothy P. Lillicrap and Jonathan J. Hunt and Alexander Pritzel and Nicolas Heess and Tom Erez and Yuval Tassa and David Silver and Daan Wierstra},
      year={2019},
      eprint={1509.02971},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1509.02971}, 
}

@misc{duan2016benchmarkingdeepreinforcementlearning,
      title={Benchmarking Deep Reinforcement Learning for Continuous Control}, 
      author={Yan Duan and Xi Chen and Rein Houthooft and John Schulman and Pieter Abbeel},
      year={2016},
      eprint={1604.06778},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1604.06778}, 
}

@misc{schulman2017proximalpolicyoptimizationalgorithms,
      title={Proximal Policy Optimization Algorithms}, 
      author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
      year={2017},
      eprint={1707.06347},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1707.06347}, 
}

@misc{haarnoja2018softactorcriticoffpolicymaximum,
      title={Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor}, 
      author={Tuomas Haarnoja and Aurick Zhou and Pieter Abbeel and Sergey Levine},
      year={2018},
      eprint={1801.01290},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1801.01290}, 
}

@book{Sutton2018,
  added-at = {2019-07-13T10:11:53.000+0200},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  biburl = {https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/lanteunis},
  edition = {Second},
  interhash = {ac6b144aaec1819919a2fba9f705c852},
  intrahash = {f46601cf8b13d39d1378af0d79438b12},
  keywords = {},
  publisher = {The MIT Press},
  timestamp = {2019-07-13T10:11:53.000+0200},
  title = {Reinforcement Learning: An Introduction},
  url = {http://incompleteideas.net/book/the-book-2nd.html},
  year = {2018 }
}

@misc{fujimoto2018addressingfunctionapproximationerror,
      title={Addressing Function Approximation Error in Actor-Critic Methods}, 
      author={Scott Fujimoto and Herke van Hoof and David Meger},
      year={2018},
      eprint={1802.09477},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1802.09477}, 
}

@article{sumalatha2024overviewddpg,
  title={An Overview Of Deep Deterministic Policy Gradient Algorithm And Applications},
  author={V. Sumalatha and Suresh Pabboju},
  journal={IOSR Journal of Computer Engineering (IOSR-JCE)},
  volume={26},
  number={5, Ser. 3},
  pages={26--28},
  year={2024},
  month={Sept.--Oct.},
  issn={2278-0661},
  note={e-ISSN: 2278-0661, p-ISSN: 2278-8727},
  url={https://www.iosrjournals.org/iosr-jce/papers/Vol26-issue5/Ser-3/D2605032628.pdf},
  abstract={This research summarizes the major developments of the Deep Deterministic Policy Gradient (DDPG) in reinforcement learning. Motivated by the ideas of Deep Q-networks, DDPG has proven capable of confronting more complex problems involving continuous action spaces. The core of DDPG lies in its actor-critic architecture, which enables the learning of highly competitive policies. By leveraging neural network function approximations, it can efficiently operate in large state and action spaces. DDPG has found practical applications across various real-world domains. However, like many model-free reinforcement learning methods, DDPG still faces the challenge of requiring a large number of training steps.},
  keywords={RL, DDPG, DQN, NN},
  submitted={2024-09-26},
  accepted={2024-10-06}
}

@inproceedings{barthmaron2018distributedddpg,
  title={Distributed Distributional Deterministic Policy Gradients},
  author={Gabriel Barth-Maron and Matthew W. Hoffman and David Budden and Will Dabney and Dan Horgan and Dhruva TB and Alistair Muldal and Nicolas Heess and Timothy Lillicrap},
  booktitle={Proceedings of the International Conference on Learning Representations (ICLR)},
  year={2018},
  organization={ICLR},
  note={Published as a conference paper at ICLR 2018},
  url={https://openreview.net/forum?id=SyZipzbCb},
  institution={DeepMind, London, UK},
  email={gabrielbm, mwhoffman, budden, wdabney, horgan, dhruvat, alimuldal, heess, countzero@google.com}
}

@misc{haarnoja2019sacapplications,
  title={Soft Actor-Critic Algorithms and Applications},
  author={Tuomas Haarnoja and Sehoon Ha and Jie Tan and Aurick Zhou and Kristian Hartikainen and Vikash Kumar and Henry Zhu and Abhishek Gupta and Pieter Abbeel and Sergey Levine},
  year={2019},
  eprint={1812.05905},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  version={v2},
  url={https://arxiv.org/abs/1812.05905v2}
}

@misc{schulman2015gae,
  title={High-Dimensional Continuous Control Using Generalized Advantage Estimation},
  author={John Schulman and Philipp Moritz and Sergey Levine and Michael I. Jordan and Pieter Abbeel},
  year={2015},
  archivePrefix={arXiv},
  eprint={1506.02438},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/1506.02438}
}

@misc{openai2021bipedalwalker,
  author       = {{OpenAI}},
  title        = {{OpenAI Gym BipedalWalker-v3 Documentation}},
  year         = {2021},
  howpublished = {\url{https://www.gymlibrary.dev/environments/box2d/bipedal_walker/}},
  note         = {Accessed: 2025-04-29}
}

@misc{pylessons2021bipedalwalker,
  author       = {{PyLessons}},
  title        = {BipedalWalker-v3 using PPO},
  year         = {2021},
  howpublished = {\url{https://pylessons.com/BipedalWalker-v3-PPO}},
  note         = {Accessed: 2025-04-29}
}

@misc{stablebaselines3docs2023,
  title        = {{Stable Baselines3 Documentation}},
  year         = {2023},
  howpublished = {\url{https://stable-baselines3.readthedocs.io/}},
  note         = {Accessed: 2025-04-29}
}

@misc{kreil2025github,
  author       = {Leonard Kreil and Zia Asmara},
  title        = {Continuous Control â€“ GitHub Repository},
  year         = {2025},
  howpublished = {\url{https://github.com/LeonardKreil/continuous-control/tree/main}},
  note         = {Accessed: 2025-04-29}
}

@misc{gottwald2018neuralvalue,
  title={Neural Value Function Approximation in Continuous State Reinforcement Learning Problems},
  author={Martin Gottwald and Mingpan Guo and Hao Shen},
  year={2018},
  howpublished={Presented at the 14th European Workshop on Reinforcement Learning (EWRL 2018), Lille, France},
  note={No formal proceedings published},
  url={https://ewrl.wordpress.com/past-ewrl/ewrl14-2018/}
}

@inproceedings{shen2024ddpgtd3walker2d,
  title={Comparison of DDPG and TD3 Algorithms in a Walker2D Scenario},
  author={Xinrui Shen},
  booktitle={Proceedings of the 2023 International Conference on Data Science, Advanced Algorithm and Intelligent Computing (DAI 2023)},
  pages={148--155},
  year={2024},
  publisher={Atlantis Press},
  doi={10.2991/978-94-6463-370-2_17},
  url={https://www.atlantis-press.com/proceedings/dai-23/125998091},
  address={Lille, France},
  month={October},
  note={Available online: 14 February 2024}
}

@inproceedings{kormushev2011policyrepresentation,
  title={Challenges for the Policy Representation when Applying Reinforcement Learning in Robotics},
  author={Petar Kormushev and Sylvain Calinon and Darwin G. Caldwell and Barkan Ugurlu},
  booktitle={Proceedings of the International Joint Conference on Neural Networks (IJCNN 2011)},
  year={2011},
  pages={2843--2850},
  organization={IEEE},
  address={Nagoya, Japan},
  url={https://calinon.ch/papers/Kormushev-IJCNN2012.pdf},
  email={petar.kormushev, sylvain.calinon, darwin.caldwell@iit.it, barkanu@toyota-ti.ac.jp},
  note={Department of Advanced Robotics, Istituto Italiano di Tecnologia, Genova, Italy, and Toyota Technological Institute, Nagoya, Japan}
}

@misc{wang2018explorationexploitation,
  title={Exploration versus Exploitation in Reinforcement Learning: A Stochastic Control Approach},
  author={Haoran Wang and Thaleia Zariphopoulou},
  year={2019},
  note={First draft: March 2018, This draft: February 2019},
  institution={Columbia University},
  url={https://arxiv.org/pdf/1812.01552},
  eprint={1902.00718},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@article{schulman2015trpo,
  title={Trust Region Policy Optimization},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1502.05477},
  year={2015}
}

@inproceedings{NEURIPS2018_3de568f8,
 author = {Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and Levine, Sergey},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/3de568f8597b94bda53149c7d7f5958c-Paper.pdf},
 volume = {31},
 year = {2018}
}

@misc{yu2021metaworldbenchmarkevaluationmultitask,
      title={Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning}, 
      author={Tianhe Yu and Deirdre Quillen and Zhanpeng He and Ryan Julian and Avnish Narayan and Hayden Shively and Adithya Bellathur and Karol Hausman and Chelsea Finn and Sergey Levine},
      year={2021},
      eprint={1910.10897},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1910.10897}, 
}

@article{wang2019neural,
  title={Neural Policy Gradient Methods: Global Optimality and Rates of Convergence},
  author={Wang, Lingxiao and Cai, Qi and Yang, Zhuoran},
  journal={arXiv preprint arXiv:1909.01150},
  year={2019},
  note={version 3, 12 Nov 2019}
}
